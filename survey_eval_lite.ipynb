{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfOfNvR6S7r0v4aOEHuC/I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/higherbar-ai/survey-eval-lite/blob/main/survey_eval_lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this survey-eval-lite notebook\n",
        "\n",
        "This notebook provides a simple example of an automated AI workflow. It's a much-simplified version of [the surveyeval toolkit available here in GitHub](https://github.com/higherbar-ai/survey-eval) designed to run in [Google Colab](https://colab.research.google.com). This version, self-contained in this single notebook, uses the OpenAI API to:\n",
        "\n",
        "1. Parse a text-format survey into a series of questions\n",
        "\n",
        "2. Loop through each question to:\n",
        "\n",
        "    1. Evaluate the question for potential phrasing\n",
        "    2. Evaluate the question for potential bias\n",
        "\n",
        "3. Assemble and output all findings and recommendations\n",
        "\n",
        "## Configuration\n",
        "\n",
        "This notebook uses secrets, which you can configure by clicking on the key icon in Google Colab's left sidebar.\n",
        "\n",
        "To use OpenAI directly, configure the `openai_api_key` secret to contain your API key. (Get a key from [the OpenAI API key page](https://platform.openai.com/api-keys), and be sure to fund your platform account with at least $5 to allow GPT-4o model access.)\n",
        "\n",
        "Alternatively, you can use OpenAI via Microsoft Azure by configuring the following secrets:\n",
        "\n",
        "1. `azure_api_key`\n",
        "2. `azure_api_base`\n",
        "3. `azure_api_engine`\n",
        "4. `azure_api_version`\n",
        "\n",
        "Finally, you can override the default model of `gpt-4o` by setting the `openai_model` secret to your preferred model, and you can optionally add [LangSmith tracing](https://langchain.com/langsmith) by setting the `langsmith_api_key` secret.\n",
        "\n",
        "## Your survey text\n",
        "\n",
        "This notebook will prompt you to upload a survey `.txt` file in plain text format (like [this short excerpt from the DHS](https://github.com/higherbar-ai/survey-eval-lite/blob/main/sample_dhs_questions.txt)). Tips on creating `.txt` versions from other formats:\n",
        "\n",
        "1. `.docx`: In Microsoft Word, click _Save as_ and then choose `Plain Text (.txt)` as the format.\n",
        "\n",
        "2. `.pdf`: Upload to [ChatGPT](https://chatgpt.com/) and ask it to give you the survey in plain text format, then copy and paste into a `.txt` file.\n",
        "\n",
        "3. `.xlsx`: Print a preview to `.pdf` format and then use ChatGPT as in #2 above.\n",
        "\n",
        "While it's certainly possible to read `.pdf` and other formats, it's a fair bit more complicated. For example, see the code and discussion in [the surveyeval toolkit](https://github.com/higherbar-ai/survey-eval)."
      ],
      "metadata": {
        "id": "IazipVYi_yUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing prerequisites\n",
        "\n",
        "This next code block installs all necessary Python packages into the current environment."
      ],
      "metadata": {
        "id": "qZuE5AxQScij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-colab\n",
        "!pip install 'langchain>=0.2.0,<0.3'\n",
        "!pip install 'langchain-openai==0.1.19'\n",
        "!pip install 'langchain-community>=0.2.0,<0.3'\n",
        "!pip install 'langsmith>=0.1.63,<0.2'\n",
        "!pip install 'tiktoken>=0.7.0,<1.0.0'\n",
        "!pip install 'openai==1.37.1'\n",
        "!pip install tenacity"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UGxo3IreHmUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing LLM and defining support functions\n",
        "\n",
        "This next code block uses your configured secrets to initialize LangChain for OpenAI access (possibly via Microsoft Azure, if secrets are configured for that).\n",
        "\n",
        "It also includes a set of handy support functions to facilitate AI workflows."
      ],
      "metadata": {
        "id": "SGJyVWA1Ua-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9coSno4_Arf"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai.chat_models.base import _AllReturnType, ChatOpenAI\n",
        "from langchain_openai.chat_models.azure import _AllReturnType, AzureChatOpenAI\n",
        "from langchain_core.messages import BaseMessage\n",
        "import concurrent.futures\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type\n",
        "import json\n",
        "import os\n",
        "\n",
        "# utility function: get secret from Google Colab, with support for a default\n",
        "def get_secret_with_default(secretName, defaultValue=None):\n",
        "  try:\n",
        "    return userdata.get(secretName)\n",
        "  except:\n",
        "    return defaultValue\n",
        "\n",
        "\n",
        "# read all supported secrets\n",
        "openai_api_key = get_secret_with_default('openai_api_key')\n",
        "openai_model = get_secret_with_default('openai_model', 'gpt-4o')\n",
        "azure_api_key = get_secret_with_default('azure_api_key')\n",
        "azure_api_base = get_secret_with_default('azure_api_base')\n",
        "azure_api_engine = get_secret_with_default('azure_api_engine')\n",
        "azure_api_version = get_secret_with_default('azure_api_version')\n",
        "langsmith_api_key = get_secret_with_default('langsmith_api_key')\n",
        "\n",
        "# complain if we don't have the bare minimum to run\n",
        "if not openai_api_key and not (azure_api_key\n",
        "                               and azure_api_base\n",
        "                               and azure_api_engine\n",
        "                               and azure_api_version):\n",
        "  raise Exception('We need either an openai_api_key secret set in the secrets — or set azure_api_key, azure_api_base, azure_api_engine, and azure_api_version to use Azure instead. Also be sure to enable Notebook Access for the secret(s).')\n",
        "\n",
        "# initialize LangSmith API (if key specified)\n",
        "if langsmith_api_key:\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = \"local\"\n",
        "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key\n",
        "\n",
        "# configure LLM settings\n",
        "temperature = 0.1\n",
        "\n",
        "# configure retry and timeout settings for calls to the LLM\n",
        "total_response_timeout_seconds = 600\n",
        "number_of_retries = 2\n",
        "seconds_between_retries = 5\n",
        "\n",
        "# initialize LangChain LLM access\n",
        "if azure_api_key:\n",
        "    llm = AzureChatOpenAI(openai_api_key=azure_api_key, temperature=temperature, deployment_name=azure_api_engine, azure_endpoint=azure_api_base,\n",
        "                          openai_api_version=azure_api_version, openai_api_type=\"azure\")\n",
        "else:\n",
        "    llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=temperature, model_name=openai_model)\n",
        "json_llm = llm.with_structured_output(method=\"json_mode\", include_raw=True)\n",
        "\n",
        "# report success\n",
        "print(\"Initialization successful.\")\n",
        "\n",
        "\n",
        "# utility function: call out to LLM for structured JSON response\n",
        "def llm_json_response(prompt) -> _AllReturnType | _AllReturnType | dict[str, BaseMessage]:\n",
        "    # execute LLM evaluation, but catch and return any exceptions\n",
        "    try:\n",
        "        result = json_llm.invoke(prompt)\n",
        "    except Exception as caught_e:\n",
        "        # format error result like success result\n",
        "        result = {\"raw\": BaseMessage(type=\"ERROR\", content=f\"{caught_e}\")}\n",
        "    return result\n",
        "\n",
        "# utility function: call out to LLM for structured JSON response, w/ automatic timeout and retry\n",
        "@retry(stop=stop_after_attempt(number_of_retries), wait=wait_fixed(seconds_between_retries),\n",
        "       retry=retry_if_exception_type(concurrent.futures.TimeoutError), reraise=True)\n",
        "def llm_json_response_with_timeout(prompt) -> _AllReturnType | _AllReturnType | dict[str, BaseMessage]:\n",
        "    try:\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            future = executor.submit(llm_json_response, prompt)\n",
        "            result = future.result(timeout=total_response_timeout_seconds)\n",
        "    except Exception as caught_e:\n",
        "        # format error result like success result\n",
        "        result = {\"raw\": BaseMessage(type=\"ERROR\", content=f\"{caught_e}\")}\n",
        "    return result\n",
        "\n",
        "# utility function: process JSON response and return as raw response and parsed dictionary from JSON\n",
        "def process_json_response(response)-> tuple[str, dict]:\n",
        "    final_response = \"\"\n",
        "    parsed_response = None\n",
        "    if response['raw'].type == \"ERROR\":\n",
        "        # if we caught an error, report and save that error, then move on\n",
        "        final_response = response['raw'].content\n",
        "        print(f\"Error from LLM: {final_response}\")\n",
        "    elif 'parsed' in response and response['parsed'] is not None:\n",
        "        # if we got a parsed version, save the JSON version of that\n",
        "        final_response = json.dumps(response['parsed'])\n",
        "        parsed_response = response['parsed']\n",
        "    elif 'parsing_error' in response and response['parsing_error'] is not None:\n",
        "        # if there was a parsing error, report and save that error, then move on\n",
        "        final_response = str(response['parsing_error'])\n",
        "        print(f\"Parsing error : {final_response}\")\n",
        "    else:\n",
        "        final_response = \"\"\n",
        "        print(f\"Unknown response from LLM\")\n",
        "\n",
        "    # return response in both raw and parsed formats\n",
        "    return final_response, parsed_response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading your survey file\n",
        "\n",
        "When you run this next code cell, it will prompt you to upload a `.txt` file with the plain text of your survey. It will then output the contents of that file so that you can confirm that it read it okay.\n",
        "\n",
        "If you don't have a `.txt` file handy, you can use [this short excerpt from the DHS](https://github.com/higherbar-ai/survey-eval-lite/blob/main/sample_dhs_questions.txt)."
      ],
      "metadata": {
        "id": "F8gvZYWeSpPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io, os\n",
        "\n",
        "# prompt for a .txt file and keep prompting till we get one\n",
        "print('Upload a .txt file with your survey text:')\n",
        "print()\n",
        "survey_text = None\n",
        "while True:\n",
        "  # prompt for upload\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  # complain if we didn't get just a single file\n",
        "  if len(uploaded.items()) != 1:\n",
        "    print()\n",
        "    print('Please upload a single .txt file.')\n",
        "    print()\n",
        "    continue\n",
        "\n",
        "  filename, data = uploaded.popitem()\n",
        "  if not filename.endswith('.txt'):\n",
        "    # clean up the unsupported file from local storage\n",
        "    os.remove(filename)\n",
        "    print()\n",
        "    print(\"Invalid file type. Please upload a .txt file.\")\n",
        "    print()\n",
        "    continue\n",
        "\n",
        "  # read the file\n",
        "  survey_text = io.StringIO(data.decode('utf-8')).read()\n",
        "  # clean up the read file from local storage\n",
        "  os.remove(filename)\n",
        "  # break from our re-prompt loop\n",
        "  break\n",
        "\n",
        "# output the result\n",
        "print()\n",
        "print(f'Survey text read from {filename}:')\n",
        "print()\n",
        "print(survey_text)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u0BAoiTHIJnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing the survey text\n",
        "\n",
        "The next code block will use the LLM to parse the survey text into a list of questions."
      ],
      "metadata": {
        "id": "elA3S_P-9_MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up our survey-parsing prompt for the LLM\n",
        "parsing_prompt = f\"\"\"You are an expert survey questionnaire and form parser. Given the plain text of a survey or digital form, you can parse into a well-structured JSON list of questions. Important instructions:\n",
        "\n",
        "* **Your job is to extract exact text from the supplied survey text:** In the JSON you return, only ever include text content, directly quoted without modification, from the survey text you are supplied (i.e., never add or invent any text and never revise or rephrase any text).\n",
        "\n",
        "* **Only respond with valid JSON that precisely follows the format specified below:** Your response should only include valid JSON and nothing else; if you cannot find any questions to return, simply return an empty questions list.\n",
        "\n",
        "* **Treat translations as separate questions:** If you see one or more translated versions of a question, include them as separate questions in the JSON you return.\n",
        "\n",
        "The JSON you return should include fields as follows:\n",
        "\n",
        "* `questions` (list): The list of questions extracted, or an empty list if none found. Each question should be an object with the following fields:\n",
        "\n",
        "  * `question_id` (string): The numeric or alphanumeric identifier or short variable name identifying the question (if any), usually located just before or at the beginning of the question. \"\" if none found.\n",
        "\n",
        "  * `question` (string): The exact text of the question or form field, including any introductory text that provides context or explanation. Often follows a unique question ID of some sort, like \"2.01.\" or \"gender:\". Should not include response options, which should be included in the 'options' field, or extra enumerator or interviewer instructions (including interview probes), which should be included in the 'instructions' field. Be careful: the same question might be asked in multiple languages, and each translation should be included as a separate question. Never translate between languages or otherwise alter the question text in any way.\n",
        "\n",
        "  * `instructions` (string): Instructions or other guidance about how to ask or answer the question (if any), including enumerator or interviewer instructions. If the question includes a list of specific response options, do NOT include those in the instructions. However, if there is guidance as to how to fill out an open-ended numeric or text response, or guidance about how to choose among the options, include that guidance here. \"\" if none found.\n",
        "\n",
        "  * `options` (string): The list of specific response options for multiple-choice questions in a single string, including both the label and the internal value (if specified) for each option. For example, a 'Male' label might be coupled with an internal value of '1', 'M', or even 'male'. Separate response options with a space, three pipe symbols ('|||'), and another space, and, if there is an internal value, add a space, three # symbols ('###'), and the internal value at the end of the label. For example: 'Male ### 1 ||| Female ### 2' (codes included) or 'Male ||| Female' (no codes); 'Yes ### yes ||| No ### no', 'Yes ### 1 ||| No ### 0', 'Yes ### y ||| No ### n', or 'YES ||| NO'. Do NOT include fill-in-the-blank content here, only multiple-choice options. \"\" if the question is open-ended (i.e., does not include specific multiple-choice options).\n",
        "\n",
        "Here is the survey text for you to parse, delimited by triple backticks:\n",
        "\n",
        "```\n",
        "{survey_text}\n",
        "```\n",
        "\n",
        "Return your JSON list of questions, each with `question_id`, `question`, `instructions`, and `options` strings:\n",
        "\"\"\"\n",
        "\n",
        "# call out to the LLM and process the returned JSON\n",
        "response_text, response_dict = process_json_response(llm_json_response_with_timeout(parsing_prompt))\n",
        "\n",
        "# output results\n",
        "if response_dict is not None:\n",
        "  # get list of questions from the response dictionary\n",
        "  questions = response_dict['questions']\n",
        "  # output summary of results\n",
        "  num_questions = len(questions)\n",
        "  num_question_ids = len(set([q['question_id'] for q in questions]))\n",
        "  num_instructions = len(set([q['instructions'] for q in questions]))\n",
        "  num_options = len(set([q['options'] for q in questions]))\n",
        "  print(f\"Parsed {num_questions} questions ({num_question_ids} with IDs, {num_instructions} with instructions, and {num_options} with multiple-choice options)\")\n",
        "else:\n",
        "  print(f\"Failed to parse any questions. Response text: {response_text}\")"
      ],
      "metadata": {
        "id": "DESt6PAj-WKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reviewing the survey questions\n",
        "\n",
        "This next code block will review each question in the survey, asking the LLM for advice re: question phrasing as well as potential biased or stereotypical language."
      ],
      "metadata": {
        "id": "CQLAfiV8MlXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through every question, reviewing them and saving results as we go\n",
        "all_results = []\n",
        "for question in questions:\n",
        "  # format our question for the LLM\n",
        "  question_text = f\"\"\"* Question ID: {question['question_id']}\n",
        "* Instructions: {question['instructions']}\n",
        "* Question: {question['question']}\n",
        "* Options: {question['options']}\"\"\"\n",
        "\n",
        "  # set up our phrasing-review prompt for the LLM\n",
        "  phrasing_prompt = f\"\"\"You are an AI designed to evaluate questionnaires and other survey instruments used by researchers and M&E professionals. You are an expert in survey methodology with training equivalent to a member of the American Association for Public Opinion Research (AAPOR) with a Ph.D. in survey methodology from University of Michigan’s Institute for Social Research. You consider primarily the content, context, and questions provided to you, and then content and methods from the most widely-cited academic publications and public and nonprofit research organizations.\n",
        "\n",
        "You always give truthful, factual answers. When asked to give your response in a specific format, you always give your answer in the exact format requested. You never give offensive responses. If you don’t know the answer to a question, you truthfully say you don’t know.\n",
        "\n",
        "You will be given the raw text from a questionnaire or survey instrument between |!| and |!| delimiters. You will also be given a specific question from that text to evaluate between |@| and |@| delimiters. The question will be supplied in the following format:\n",
        "\n",
        "* Question ID: ID (if any)\n",
        "* Instructions: Instructions (if any)\n",
        "* Question: Question text\n",
        "* Options: Multiple-choice options (if any), with each separated by three pipe symbols (|||) and option values (if any) separated from option labels by three hash symbols (###)\n",
        "\n",
        "Evaluate the question only, but also consider its context within the larger survey.\n",
        "\n",
        "Assume that this survey will be administered by a trained enumerator who asks each question and reads each prompt or instruction as indicated in the excerpt. Your job is to anticipate the phrasing or translation issues that would be identified in a rigorous process of pre-testing (with cognitive interviewing) and piloting.\n",
        "\n",
        "When evaluating the question, DO:\n",
        "\n",
        "1. Ensure that the question will be understandable by substantially all respondents.\n",
        "\n",
        "2. Consider the question in the context of the excerpt, including any instructions, related questions, or prompts that precede it.\n",
        "\n",
        "3. Ignore question numbers and formatting.\n",
        "\n",
        "4. Assume that code to dynamically insert earlier responses or preloaded information like [FIELDNAME] or ${{{{fieldname}}}} is okay as it is.\n",
        "\n",
        "5. Ignore HTML or other formatting, and focus solely on question phrasing (assume that HTML tags will be for visual formatting only and will not be read aloud).\n",
        "\n",
        "When evaluating the question, DON'T:\n",
        "\n",
        "1. Recommend translating something into another language (i.e., suggestions for rephrasing should always be in the same language as the original text).\n",
        "\n",
        "2. Recommend changes in the overall structure of a question (e.g., changing from multiple choice to open-ended or splitting one question into multiple), unless it will substantially improve the quality of the data collected.\n",
        "\n",
        "3. Comment on HTML tags or formatting.\n",
        "\n",
        "Respond in JSON format with all of the following fields:\n",
        "\n",
        "* `Phrases` (list): a list containing all phrases from the excerpt that pre-testing or piloting is likely to identify as problematic (each phrase should be an exact quote)\n",
        "\n",
        "* `Number of phrases` (number): the exact number of phrases in Phrases [ Note that this key must be exactly \"Number of phrases\", with exactly that capitalization and spacing ]\n",
        "\n",
        "* `Recommendations` (list): a list containing suggested replacement phrases, one for each of the phrases in Phrases (in the same order as Phrases; each replacement phrase should be an exact quote that can exactly replace the corresponding phrase in Phrases; and each replacement phrase should be in the same language as the original phrase)\n",
        "\n",
        "* `Explanations` (list): a list containing explanations for why the authors should consider revising each phrase, one for each of the phrases in Phrases (in the same order as Phrases). Do not repeat the entire phrase in the explanation, but feel free to reference specific words or parts as needed.\n",
        "\n",
        "* `Severities` (list): a list containing the severity of each identified issue, one for each of the phrases in Phrases (in the same order as Phrases); each severity should be expressed as a number on a scale from 1 for the least severe issues (minor phrasing issues that are very unlikely to substantively affect responses) to 5 for the most severe issues (problems that are likely to substantively affect responses in a way that introduces bias and/or variance)\n",
        "\n",
        "Raw text:\n",
        "|!|\n",
        "{survey_text}\n",
        "|!|\n",
        "\n",
        "Question to evaluate:\n",
        "|@|\n",
        "{question_text}\n",
        "|@|\n",
        "\n",
        "Your JSON response following the format described above:\"\"\"\n",
        "\n",
        "  # call out to the LLM\n",
        "  print()\n",
        "  print(f\"Evaluating question for phrasing: {question['question']}\")\n",
        "  response_text, response_dict = process_json_response(llm_json_response_with_timeout(phrasing_prompt))\n",
        "\n",
        "  # save and output results\n",
        "  if response_dict is not None:\n",
        "    if 'Number of phrases' in response_dict and response_dict['Number of phrases'] > 0:\n",
        "      print(f\"  Identified {response_dict['Number of phrases']} issue(s)\")\n",
        "      all_results += [response_dict]\n",
        "    else:\n",
        "      print(\"  No issues identified\")\n",
        "  else:\n",
        "    print(f\"  Failed to get a valid response. Response text: {response_text}\")\n",
        "\n",
        "  # set up our bias-review prompt for the LLM\n",
        "  # Note that this prompt was inspired by the example in this blog post:\n",
        "  # https://www.linkedin.com/pulse/using-chatgpt-counter-bias-prejudice-discrimination-johannes-schunter/\n",
        "  bias_prompt = f\"\"\"You are an AI designed to evaluate questionnaires and other survey instruments used by researchers and M&E professionals. You are an expert in survey methodology with training equivalent to a member of the American Association for Public Opinion Research (AAPOR) with a Ph.D. in survey methodology from University of Michigan’s Institute for Social Research. You are also an expert in the areas of gender equality, discrimination, anti-racism, and anti-colonialism. You consider primarily the content, context, and questions provided to you, and then content and methods from the most widely-cited academic publications and public and nonprofit research organizations.\n",
        "\n",
        "You always give truthful, factual answers. When asked to give your response in a specific format, you always give your answer in the exact format requested. You never give offensive responses. If you don’t know the answer to a question, you truthfully say you don’t know.\n",
        "\n",
        "You will be given the raw text from a questionnaire or survey instrument between |!| and |!| delimiters. You will also be given a specific question from that text to evaluate between |@| and |@| delimiters. The question will be supplied in the following format:\n",
        "\n",
        "* Question ID: ID (if any)\n",
        "* Instructions: Instructions (if any)\n",
        "* Question: Question text\n",
        "* Options: Multiple-choice options (if any), with each separated by three pipe symbols (|||) and option values (if any) separated from option labels by three hash symbols (###)\n",
        "\n",
        "Evaluate the question only, but also consider its context within the larger survey.\n",
        "\n",
        "Assume that this survey will be administered by a trained enumerator who asks each question and reads each prompt or instruction as indicated in the excerpt. Your job is to review the question for:\n",
        "\n",
        "a. Stereotypical representations of gender, ethnicity, origin, religion, or other social categories.\n",
        "\n",
        "b. Distorted or biased representations of events, topics, groups, or individuals.\n",
        "\n",
        "c. Use of discriminatory or insensitive language towards certain groups or topics.\n",
        "\n",
        "d. Implicit or explicit assumptions made in the text or unquestioningly adopted that could be based on prejudices.\n",
        "\n",
        "e. Prejudiced descriptions or evaluations of abilities, characteristics, or behaviors.\n",
        "\n",
        "Respond in JSON format with all of the following fields:\n",
        "\n",
        "* `Phrases`: a list containing all problematic phrases from the excerpt that you found in your review (each phrase should be an exact quote from the excerpt)\n",
        "\n",
        "* `Number of phrases`: the exact number of phrases in Phrases [ Note that this key must be exactly \"Number of phrases\", with exactly that capitalization and spacing ]\n",
        "\n",
        "* `Recommendations`: a list containing suggested replacement phrases, one for each of the phrases in Phrases (in the same order as Phrases; each replacement phrase should be an exact quote that can exactly replace the corresponding phrase in Phrases)\n",
        "\n",
        "* `Explanations`: a list containing explanations for why the phrases are problematic, one for each of the phrases in Phrases (in the same order as Phrases)\n",
        "\n",
        "* `Severities`: a list containing the severity of each identified issue, one for each of the phrases in Phrases (in the same order as Phrases); each severity should be expressed as a number on a scale from 1 for the least severe issues (minor phrasing issues that are very unlikely to offend respondents or substantively affect their responses) to 5 for the most severe issues (problems that are very likely to offend respondents or substantively affect responses in a way that introduces bias and/or variance)\n",
        "\n",
        "Raw text:\n",
        "|!|\n",
        "{survey_text}\n",
        "|!|\n",
        "\n",
        "Question to evaluate:\n",
        "|@|\n",
        "{question_text}\n",
        "|@|\n",
        "\n",
        "Your JSON response following the format described above:\"\"\"\n",
        "\n",
        "  # call out to the LLM\n",
        "  print()\n",
        "  print(f\"Evaluating question for bias: {question['question']}\")\n",
        "  response_text, response_dict = process_json_response(llm_json_response_with_timeout(bias_prompt))\n",
        "\n",
        "  # save and output results\n",
        "  if response_dict is not None:\n",
        "    if 'Number of phrases' in response_dict and response_dict['Number of phrases'] > 0:\n",
        "      print(f\"  Identified {response_dict['Number of phrases']} issue(s)\")\n",
        "      all_results += [response_dict]\n",
        "    else:\n",
        "      print(\"  No issues identified\")\n",
        "  else:\n",
        "    print(f\"  Failed to get a valid response. Response text: {response_text}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XfEULWNH_2_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organizing and outputting the results\n",
        "\n",
        "This final code block organizes and outputs final results, saving them in a local file called `survey-review-results.txt`. View or download this file by clicking the file-folder icon in Google Colab's left sidebar."
      ],
      "metadata": {
        "id": "trJ9EDFCWQrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate report\n",
        "if len(all_results) == 0:\n",
        "  report = \"No results to save\"\n",
        "else:\n",
        "  report = \"Survey review results:\\n\"\n",
        "  for result in all_results:\n",
        "    if 'Phrases' in result and result['Number of phrases'] > 0:\n",
        "      # loop through all recommendations, treating lists as parallel arrays\n",
        "      for phrase, recommendation, explanation, severity in zip(result['Phrases'], result['Recommendations'], result['Explanations'], result['Severities']):\n",
        "        report += f\"\\n---\\n\\n{explanation}\\n\\nImportance: {severity} out of 5\\n\\nSuggest replacing this: {phrase}\\n\\nWith this: {recommendation}\\n\"\n",
        "\n",
        "# save the report to file\n",
        "with open(\"survey-review-results.txt\", \"w\") as f:\n",
        "  f.write(report)\n",
        "\n",
        "print(\"All recommendations saved to survey-review-results.txt\")"
      ],
      "metadata": {
        "id": "wqn1yzYsWfq5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}